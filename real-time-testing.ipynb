{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84568d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing with video feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbfed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from plyer import notification\n",
    "\n",
    "# Load YOLOv4 model and class labels\n",
    "net = cv2.dnn.readNetFromDarknet('yolov4_custom_1.cfg', 'yolov4_custom_12000.weights')\n",
    "classes = ['Cycling specific sign', 'Directional sign', 'Hazard sign', 'Stop sign', 'Warning sign', 'Pedestrian sign', 'Zebra Crossing']\n",
    "\n",
    "# Create video capture object\n",
    "video_path = r'C:\\Users\\Bhoyee-pc\\Downloads\\pro-sch\\sample_data\\videos\\video5.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "output_path = r'C:\\Users\\Bhoyee-pc\\Downloads\\pro-sch\\sample_data\\videos\\output.mp4'\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (1280, 720))\n",
    "\n",
    "notification_triggered = False\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read frame from the video feed\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize frame\n",
    "    frame = cv2.resize(frame, (1280, 720))\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Preprocess frame for object detection\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255, (416, 416), (0, 0, 0), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layerOutputs = net.forward(net.getUnconnectedOutLayersNames())\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "\n",
    "    for output in layerOutputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.7:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    colors = np.random.uniform(0, 255, size=(len(boxes), 3))\n",
    "\n",
    "    if len(indexes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            confidence = str(round(confidences[i], 2))\n",
    "            color = colors[i]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, label + \" \" + confidence, (x, y + 20), font, 2, color, 2)\n",
    "            \n",
    "            # Display notification when sign is detected\n",
    "            if not notification_triggered and x > 5 and y > 5:\n",
    "                notification_title = \"Sign Detected\"\n",
    "                notification_text = f\"A {label} sign was detected.\"\n",
    "                notification.notify(title=notification_title, message=notification_text, timeout=5)\n",
    "                notification_triggered = True\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "# Release video capture object, VideoWriter object, and close windows\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e28ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing with camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9549738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from plyer import notification\n",
    "\n",
    "# Load YOLOv4 model and class labels\n",
    "net = cv2.dnn.readNetFromDarknet('yolov4_custom_1.cfg', 'yolov4_custom_12000.weights')\n",
    "classes = ['Cycling specific sign', 'Directional sign', 'Hazard sign', 'Stop sign', 'Warning sign', 'Pedestrian sign', 'Zebra Crossing']\n",
    "\n",
    "# Create video capture object\n",
    "cap = cv2.VideoCapture(0)  # Use index 0 for the default camera\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "output_path = os.path.join('sample_data', 'videos', 'output.mp4')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (1280, 720))\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read frame from the video feed\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize frame\n",
    "    frame = cv2.resize(frame, (1280, 720))\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Preprocess frame for object detection\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255, (416, 416), (0, 0, 0), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layerOutputs = net.forward(net.getUnconnectedOutLayersNames())\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "\n",
    "    for output in layerOutputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.7:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    colors = np.random.uniform(0, 255, size=(len(boxes), 3))\n",
    "\n",
    "    # Display bounding boxes and labels\n",
    "    detected_signs = []  # List to store the detected signs\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            confidence = str(round(confidences[i], 2))\n",
    "            color = colors[i]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, label + \" \" + confidence, (x, y + 20), font, 2, color, 2)\n",
    "\n",
    "            # Add the detected sign to the list\n",
    "            detected_signs.append(label)\n",
    "\n",
    "    # Display notification when road signs are detected\n",
    "    if detected_signs:\n",
    "        notification_title = \"Sign Detected\"\n",
    "        notification_text = f\"A {', '.join(detected_signs)} sign(s) was detected.\"\n",
    "        notification.notify(title=notification_title, message=notification_text, timeout=5)\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "# Release video capture object, VideoWriter object, and close windows\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9470888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "showling live camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d93fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from plyer import notification\n",
    "\n",
    "# Load YOLOv4 model and class labels\n",
    "net = cv2.dnn.readNetFromDarknet('yolov4_custom_1.cfg', 'yolov4_custom_12000.weights')\n",
    "classes = ['Cycling specific sign', 'Directional sign', 'Hazard sign', 'Stop sign', 'Warning sign', 'Pedestrian sign', 'Zebra Crossing']\n",
    "\n",
    "# Create video capture object\n",
    "cap = cv2.VideoCapture(0)  # Use index 0 for the default camera\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "output_path = 'output.mp4'\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (1280, 720))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize frame\n",
    "    frame = cv2.resize(frame, (1280, 720))\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Preprocess frame for object detection\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255, (416, 416), (0, 0, 0), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layerOutputs = net.forward(net.getUnconnectedOutLayersNames())\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "\n",
    "    for output in layerOutputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.7:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    colors = np.random.uniform(0, 255, size=(len(boxes), 3))\n",
    "\n",
    "    # Display bounding boxes and labels\n",
    "    detected_signs = []  # List to store the detected signs\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            confidence = str(round(confidences[i], 2))\n",
    "            color = colors[i]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, label + \" \" + confidence, (x, y + 20), font, 2, color, 2)\n",
    "\n",
    "            # Add the detected sign to the list\n",
    "            detected_signs.append(label)\n",
    "\n",
    "    # Display notification when road signs are detected\n",
    "    if detected_signs:\n",
    "        notification_title = \"Sign Detected\"\n",
    "        notification_text = f\"A {', '.join(detected_signs)} sign(s) was detected.\"\n",
    "        notification.notify(title=notification_title, message=notification_text, timeout=5)\n",
    "\n",
    "    cv2.imshow(\"Object Detection\", frame)\n",
    "    out.write(frame)\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture object, VideoWriter object, and close windows\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing voice with life camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b2f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import pyttsx3\n",
    "\n",
    "# Load YOLOv4 model and class labels\n",
    "net = cv2.dnn.readNetFromDarknet('yolov4_custom_1.cfg', 'yolov4_custom_12000.weights')\n",
    "classes = ['Cycling specific sign', 'Directional sign', 'Hazard sign', 'Stop sign', 'Warning sign', 'Pedestrian sign', 'Zebra Crossing']\n",
    "\n",
    "# Initialize the text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Function to perform object detection on each frame\n",
    "\"\"\"\n",
    "This function performs object detection on each frame of the video stream. \n",
    "The frame is preprocessed for object detection, and the YOLOv4 model is used to detect objects.\n",
    "The bounding boxes and labels are drawn on the frame, and the detected signs are stored in a list. \n",
    "If signs are detected, a notification text is created, and the text is spoken using the text-to-speech engine.\n",
    "\"\"\"\n",
    "def detect_objects(frame):\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Preprocess frame for object detection\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255, (416, 416), (0, 0, 0), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layerOutputs = net.forward(net.getUnconnectedOutLayersNames())\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "\n",
    "    for output in layerOutputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.7:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    colors = np.random.uniform(0, 255, size=(len(boxes), 3))\n",
    "\n",
    "    # Display bounding boxes and labels\n",
    "    detected_signs = []  # List to store the detected signs\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            confidence = str(round(confidences[i], 2))\n",
    "            color = colors[i]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, label + \" \" + confidence, (x, y + 20), font, 2, color, 2)\n",
    "\n",
    "            # Add the detected sign to the list\n",
    "            detected_signs.append(label)\n",
    "\n",
    "    # Speak the notification when road signs are detected\n",
    "    if detected_signs:\n",
    "        notification_text = f\"A {', '.join(detected_signs)} sign(s) was detected.\"\n",
    "        engine.say(notification_text)\n",
    "        engine.runAndWait()\n",
    "\n",
    "    return frame\n",
    "\n",
    "# Function to handle the video stream\n",
    "\"\"\"\n",
    "This function handles the video stream. It captures video frames from the PC camera using cv2.VideoCapture\n",
    "with index 0. You can change the index if you have multiple cameras connected. \n",
    "The function reads frames from the camera, calls the detect_objects function to perform object detection, \n",
    "displays the processed frames, and exits when the 'q' key is pressed.\n",
    "\"\"\"\n",
    "def handle_video_stream():\n",
    "    cap = cv2.VideoCapture(0)  # Open the PC camera (change the index if multiple cameras are available)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = detect_objects(frame)\n",
    "        cv2.imshow('Video Stream', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the function to start video streaming and object detection\n",
    "handle_video_stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ec996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d6b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdfb14e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
